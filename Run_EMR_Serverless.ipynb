{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331630f6-ef3b-4a4c-8c5b-c2adc57f9fef",
   "metadata": {},
   "source": [
    "#### Notebook example to run EMR serverless job from Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdfcf4-9e90-43a0-adce-0450e4752062",
   "metadata": {
    "tags": []
   },
   "source": [
    "us-east-1 applications: <br>\n",
    "* pd-autoencoder-ad-v1 : 00f64bef5869kl09\n",
    "* pd-autoencoder-ad-v2 : 00f66ohicnjchu09\n",
    "* pd-autoencoder-ad-v3 : 00f6959e87i48609\n",
    "<br>\n",
    "\n",
    "us-west-2 applications: <br>\n",
    "* pd-autoencoder-ad-container-v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d6865-7d75-4652-886e-094c61820419",
   "metadata": {},
   "source": [
    "Note: while launching your job, please make sure of the region from where you are running it.\n",
    "jobs for us-east-region can only be launched from us-east-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce93b67-f1d2-46ea-8e28-b233cf5d689b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from emr_serverless import EMRServerless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99351a9c-c5a2-4206-93f8-73b6267d48c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# application id\n",
    "application_id = '00f66mmuts7enm09'\n",
    "# don't chnage the serverless_job_role_arn\n",
    "serverless_job_role_arn = 'arn:aws:iam::064047601590:role/hamza-emr-serverless-role'\n",
    "# s3 bukcet name where the dependencies, logs and code sits\n",
    "s3_bucket_name = 'emr-serverless-output-pd'\n",
    "emr_emtry_point = 's3://emr-serverless-output-pd/code/pyspark/pd-autoencoder-ad/s3_test_emr.py'\n",
    "zipped_env_path = 's3://emr-serverless-output-pd/code/pyspark/pd-autoencoder-ad/pyspark_deps_all_rec_types_v2.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20049dd-9d42-482f-92c3-c62644141938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emr_serverless = EMRServerless(application_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9b1929-dfc3-4b08-85bd-7c377d07df1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EMR Serverless Spark App\n",
      "EMR Serverless SPARK Application: 00f66mmuts7enm09\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting EMR Serverless Spark App\")\n",
    "#emr_serverless.create_application(\"pd-autoencoder-test-emr-cli\", \"emr-6.6.0\")\n",
    "emr_serverless.start_application(application_id)\n",
    "print(emr_serverless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "656f6be5-4157-4451-97aa-a33f20f4bd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting new Spark job\n"
     ]
    }
   ],
   "source": [
    "# Run (and wait for) a Spark job\n",
    "print(\"Submitting new Spark job\")\n",
    "job_run_id = emr_serverless.run_spark_job(\n",
    "    script_location=emr_emtry_point,\n",
    "    job_role_arn=serverless_job_role_arn,\n",
    "    application_id = application_id,\n",
    "    arguments=[f\"s3://{s3_bucket_name}/emr-serverless/output\"],\n",
    "    s3_bucket_name=s3_bucket_name,\n",
    "    zipped_env_path = zipped_env_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d80ac16-e41a-42db-917c-9bd4716833d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job finished: 00f697tg11ptuk09, status is: SUCCESS\n",
      "File output from stdout.gz:\n",
      "----\n",
      " 2022-12-12 20:18:25.485882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 20:18:25.619662: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native\n",
      "2022-12-12 20:18:25.619692: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-12 20:18:26.373600: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native\n",
      "2022-12-12 20:18:26.373689: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native\n",
      "2022-12-12 20:18:26.373702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Dummy df write successful to s3 using spark context from Pyspark_data_ingestionn\n",
      " \n",
      "----\n",
      "Done! ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "job_status = emr_serverless.get_job_run(job_run_id)\n",
    "print(f\"Job finished: {job_run_id}, status is: {job_status.get('state')}\")\n",
    "\n",
    "# Fetch and print the logs\n",
    "spark_driver_logs = emr_serverless.fetch_driver_log(s3_bucket_name, job_run_id)\n",
    "print(\"File output from stdout.gz:\\n----\\n\", spark_driver_logs, \"\\n----\")\n",
    "\n",
    "# Now stop and delete your application\n",
    "#print(\"Stopping App\")\n",
    "#emr_serverless.stop_application()\n",
    "#emr_serverless.delete_application()\n",
    "print(\"Done! ðŸ‘‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59092a7-d5fb-471b-a1dc-5d1bc0ecfabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
