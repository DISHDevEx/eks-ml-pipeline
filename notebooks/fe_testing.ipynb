{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f9216f-62fa-4328-9d1f-57208907023c",
   "metadata": {},
   "source": [
    "## testing for feature engineering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ae8e08-59ca-4533-81e8-f447b22dfcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Processing /root/git/msspackages/dist/msspackages-0.0.7-py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (1.3.5)\n",
      "Requirement already satisfied: dask in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (2022.2.0)\n",
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (3.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (1.21.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (4.42.1)\n",
      "Requirement already satisfied: configparser in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (5.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (2.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (0.10.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (2022.11.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (1.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->msspackages==0.0.7) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->msspackages==0.0.7) (2019.3)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark->msspackages==0.0.7) (0.10.9.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->dask->msspackages==0.0.7) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->dask->msspackages==0.0.7) (1.14.0)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.7/site-packages (from partd>=0.3.10->dask->msspackages==0.0.7) (0.2.0)\n",
      "msspackages is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mb\"Hit:1 http://security.debian.org/debian-security buster/updates InRelease\\nHit:2 http://deb.debian.org/debian buster InRelease\\nHit:3 http://deb.debian.org/debian buster-updates InRelease\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\n53 packages can be upgraded. Run 'apt list --upgradable' to see them.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\nsudo is already the newest version (1.8.27-1+deb10u4).\\n0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\ndefault-jre is already the newest version (2:1.11-71).\\n0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\npython3 is already the newest version (3.7.3-1).\\n0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\npython3-pip is already the newest version (18.1-5).\\n0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install /root/git/msspackages/dist/msspackages-0.0.7-py3-none-any.whl\n",
    "from msspackages import setup_runner\n",
    "setup_runner(setup_type = 'notebook' , project = 'understanding-eks-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3dc8106-7d43-4e37-953c-e64506e2981d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d49ae8e-828e-4ad3-86cd-0993aabe2d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-2ecb50de1048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnull_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmsspackages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyspark_data_ingestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from ..utilities import feature_processor, null_report\n",
    "from msspackages import Pyspark_data_ingestion, get_features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397de37-4d8b-41c3-9a3b-25c39f46310a",
   "metadata": {},
   "source": [
    "resolved by os.chdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566b101-870a-4758-8ac4-a4f9256ab843",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e09e777b-2a8b-4cca-a75a-59106f7f998e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def node_hmm_fe_v2(feature_group_name, feature_group_version, input_year, input_month, input_day, input_hour, input_setup = \"default\"):\n",
    "\n",
    "    node_data = Pyspark_data_ingestion(\n",
    "        year = input_year, \n",
    "        month = input_month, \n",
    "        day = input_day, \n",
    "        hour = input_hour, \n",
    "        setup = input_setup, \n",
    "        filter_column_value ='Node')\n",
    "    err, node_df = node_data.read()\n",
    "    node_df = node_df.select(\"InstanceId\",'Timestamp','node_cpu_utilization','node_memory_utilization')\n",
    "\n",
    " \n",
    "    if err == 'PASS':\n",
    "        \n",
    "        #get features\n",
    "        features_df = get_features(feature_group_name,feature_group_version)\n",
    "        features = features_df[\"feature_name\"].to_list()\n",
    "        processed_features = feature_processor.cleanup(features)\n",
    "        \n",
    "        model_parameters = features_df[\"model_parameters\"].iloc[0]\n",
    "  \n",
    "        #drop na values in node cpu and memory utilization\n",
    "        node_df = node_df.select(\"InstanceId\",\"Timestamp\", *processed_features)\n",
    "        node_df = node_df.na.drop(subset=processed_features)\n",
    "        \n",
    "        #remove nodes which has a time gap over 2 minutes (epochtime = 2*60*1000=120000)\n",
    "        w = Window.partitionBy('InstanceId').orderBy('Timestamp')\n",
    "        node_df = node_df.withColumn('lead', f.lag('Timestamp', 1).over(w)) \\\n",
    "              .withColumn(\n",
    "                'Timediff', \n",
    "                f.when(f.col('lead').isNotNull(), \n",
    "                f.col('Timestamp') - f.col('lead'))\n",
    "                .otherwise(f.lit(None)))\n",
    "               \n",
    "        \n",
    "        temp_df = node_df\\\n",
    "            .groupby(\"InstanceId\")\\\n",
    "            .max(\"Timediff\")\\\n",
    "            .select('InstanceId',f.col('max(TimeDiff)').alias('maxDiff'))\\\n",
    "            .filter(\"maxDiff<=120000\")\n",
    "                                                             \n",
    "        node_df = node_df.filter(col(\"InstanceId\").isin(temp_df['InstanceId']))\n",
    "        node_df = node_df.sort(\"InstanceId\",\"Timestamp\")\n",
    "        node_df = node_df.select('InstanceId','Timestamp','node_cpu_utilization','node_memory_utilization')\n",
    "        \n",
    "        #Drop rows with nans \n",
    "        node_df = node_df.na.drop(\"all\")\n",
    "           \n",
    "        \n",
    "        return node_df\n",
    "    \n",
    "    else:\n",
    "        empty_df = pd.DataFrame()\n",
    "        return empty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ca737-6909-45a5-a8a3-a9c4132d44c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test code trunks in the function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e9aa24c-e4af-4329-bd44-a12367f8746b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_data = Pyspark_data_ingestion(\n",
    "    year =2022,\n",
    "    month = 7, \n",
    "    day = 10, \n",
    "    hour = 10, \n",
    "    setup = '128gb', \n",
    "    filter_column_value ='Node')\n",
    "err, node_df = node_data.read()\n",
    "node_df = node_df.select(\"InstanceId\",'Timestamp','node_cpu_utilization','node_memory_utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05bcba33-62bb-4ca6-ac0c-7948e9205678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_features = ['node_cpu_utilization','node_memory_utilization']\n",
    "node_df = node_df.na.drop(subset=processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ad8bcce-db21-41cd-9b59-b07b6b062c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove nodes which has a time gap over 2 minutes (epochtime = 2*60*1000=120000)\n",
    "w = Window.partitionBy('InstanceId').orderBy('Timestamp')\n",
    "node_df = node_df.withColumn('lead', f.lag('Timestamp', 1).over(w)) \\\n",
    "      .withColumn(\n",
    "        'Timediff', \n",
    "        f.when(f.col('lead').isNotNull(), \n",
    "        f.col('Timestamp') - f.col('lead'))\n",
    "        .otherwise(f.lit(None)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2167cc9-737a-405f-aba9-da17efc1e01e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "temp_df = node_df\\\n",
    "    .groupby(\"InstanceId\")\\\n",
    "    .max(\"Timediff\")\\\n",
    "    .select('InstanceId',f.col('max(TimeDiff)').alias('maxDiff'))\\\n",
    "    .filter(\"maxDiff<=120000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4bc7f7c-a583-40e3-9c49-bcb2b59ed5ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_df = node_df.filter(col(\"InstanceId\").isin(temp_df['InstanceId']))\n",
    "node_df = node_df.sort(\"InstanceId\",\"Timestamp\")\n",
    "node_df = node_df.select('InstanceId','Timestamp','node_cpu_utilization','node_memory_utilization')\n",
    "        \n",
    "#Drop rows with nans \n",
    "node_df = node_df.na.drop(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f0ab4-bd54-4075-bbfe-1a903e5d338c",
   "metadata": {},
   "source": [
    "## Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98fcc5d5-e830-4ee6-8a68-7d396604a17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def node_hmm_train_test_split(input_df,split = 0.5):\n",
    "    \n",
    "    temp_df = input_df.select('InstanceId')\n",
    "    node_train_id, node_test_id = temp_df.randomSplit(weights=[split,1-split], seed=200)  \n",
    "    node_train = input_df.filter(col(\"InstanceId\").isin(node_train_id['InstanceId']))\n",
    "    node_test = input_df.filter(col(\"InstanceId\").isin(node_test_id['InstanceId']))\n",
    "    \n",
    "    return node_train, node_test\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13177325-d137-44ef-b41c-162e3fe2a949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_train, node_test = node_hmm_train_test_split(node_df,split = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8b2f1-ee3b-46f6-89cb-fe3f79d20ab9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55506a0c-0ef0-4b11-87d2-ac4416dfd535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InstanceId', 'Timestamp', 'node_cpu_utilization', 'node_memory_utilization']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c6ba5-c17c-4593-b390-c00a76630b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = node_df.select('InstanceId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ffd245d-75f3-4df3-99c6-aab4f60fe198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_train, node_test = temp_df.randomSplit(weights=[0.5,0.5], seed=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2bfbd7-4859-4e59-af1c-b6c2e8dad370",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5c23306-51e6-4d9a-92da-4c6e172af8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def node_hmm_feature_engineer(input_df):\n",
    "    \n",
    "    #sort data\n",
    "    input_df = input_df.sort('InstanceId','Timestamp')\n",
    "    \n",
    "    #get features\n",
    "    features_df = get_features(feature_group_name,feature_group_version)\n",
    "    features = features_df[\"feature_name\"].to_list()\n",
    "    \n",
    "    #standardize feature data from the node\n",
    "    features = ['node_cpu_utilization','node_memory_utilization']\n",
    "    w = Window.partitionBy('InstanceId')\n",
    "    for c in features:\n",
    "        input_df = (input_df.withColumn('mean', f.min(c).over(w))\n",
    "            .withColumn('std', f.max(c).over(w))\n",
    "            .withColumn(c, ((f.col(c) - f.col('mean')) / (f.col('std'))))\n",
    "            .drop('mean')\n",
    "            .drop('std'))\n",
    "        \n",
    "    #standard scale the data\n",
    "    vecAssembler = VectorAssembler(inputCols=[\"node_cpu_utilization\", \"node_memory_utilization\"], outputCol=\"features\")\n",
    "    node_train = vecAssembler.transform(node_train)\n",
    "    node_train = node_train.select('InstanceId','features')\n",
    "        \n",
    "    #transfer data to a nested list (#timestamps * #features for each node)\n",
    "    instance_list = node_train.select('InstanceId').distinct()\n",
    "    features_list = []\n",
    "    for instance in instance_list:\n",
    "        sub = node_train.filter(node_train.InstanceId == instance)\n",
    "        sub_features = np.array(sub.select(\"features\").collect())\n",
    "        features_list.append(sub_features)\n",
    "                                \n",
    "    return features_list\n",
    "    \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35546c60-0efe-4c57-9da2-2ae23542df06",
   "metadata": {},
   "source": [
    "#### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7b753c8-b49c-43af-a64b-66ee79c0c2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_train = node_train.sort(\"InstanceId\",\"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc0ae0d8-aac3-4452-a72f-1522b4d9b978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['node_cpu_utilization','node_memory_utilization']\n",
    "w = Window.partitionBy('InstanceId')\n",
    "for c in features:\n",
    "    node_train = (node_train.withColumn('mean', f.mean(c).over(w))\n",
    "        .withColumn('std', f.stddev(c).over(w))\n",
    "        .withColumn(c, ((f.col(c) - f.col('mean')) / (f.col('std'))))\n",
    "        .drop('mean')\n",
    "        .drop('std'))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73eb1781-7d79-4437-b6f2-f3374b6d9960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = np.array(node_train.select(\"features\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f8942188-de6d-441d-b956-d85081baf4b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_list = node_train.select('InstanceId').distinct()\n",
    "features_list = []\n",
    "for instance in instance_list:\n",
    "    sub = node_train.filter(node_train.InstanceId == instance)\n",
    "    sub_features = np.array(sub.select(\"features\").collect())\n",
    "    features_list.append(sub_features)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b17b673c-6395-47c7-87ef-373f68d33f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.09807653,  1.32735159]],\n",
       "\n",
       "       [[ 0.92183588, -0.1294619 ]],\n",
       "\n",
       "       [[-0.97546894, -0.00512365]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.49853875,  0.61395217]],\n",
       "\n",
       "       [[-0.0091358 , -0.9719207 ]],\n",
       "\n",
       "       [[-0.09693444,  0.86475014]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee039435-d938-4ecd-a97a-4b214bd9bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_list = node_train.select('InstanceId').distinct()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
