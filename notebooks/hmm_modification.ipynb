{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411348de-7828-40bb-bce3-83be2b1e0c9b",
   "metadata": {},
   "source": [
    "## Serves as a advanced node model which focuses on building model on multiple nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f5b83-2985-4f65-b062-c615a9d0e088",
   "metadata": {
    "tags": []
   },
   "source": [
    "### import packages and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d810c2ea-6564-47e9-8cf2-c2eea8af448c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Processing /root/git/msspackages/dist/msspackages-0.0.7-py3-none-any.whl\n",
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (3.3.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (1.3.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (1.21.6)\n",
      "Requirement already satisfied: dask in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (2022.2.0)\n",
      "Requirement already satisfied: configparser in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (5.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from msspackages==0.0.7) (4.42.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (1.1.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (0.10.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (2.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (6.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from dask->msspackages==0.0.7) (20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->msspackages==0.0.7) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->msspackages==0.0.7) (2019.3)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark->msspackages==0.0.7) (0.10.9.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->dask->msspackages==0.0.7) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->dask->msspackages==0.0.7) (2.4.6)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.7/site-packages (from partd>=0.3.10->dask->msspackages==0.0.7) (0.2.0)\n",
      "msspackages is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mb\"Hit:1 http://security.debian.org/debian-security buster/updates InRelease\\nHit:2 http://deb.debian.org/debian buster InRelease\\nHit:3 http://deb.debian.org/debian buster-updates InRelease\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\n51 packages can be upgraded. Run 'apt list --upgradable' to see them.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\nsudo is already the newest version (1.8.27-1+deb10u4).\\n0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\ndefault-jre is already the newest version (2:1.11-71).\\n0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\npython3 is already the newest version (3.7.3-1).\\n0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\\nReading package lists...\\nBuilding dependency tree...\\nReading state information...\\npython3-pip is already the newest version (18.1-5).\\n0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install /root/git/msspackages/dist/msspackages-0.0.7-py3-none-any.whl\n",
    "from msspackages import setup_runner\n",
    "setup_runner(setup_type = 'notebook' , project = 'understanding-eks-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbd307c-b2e5-42b4-9a37-8c515fc2fb79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: hmmlearn in /opt/conda/lib/python3.7/site-packages (0.2.8)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /opt/conda/lib/python3.7/site-packages (from hmmlearn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.7/site-packages (from hmmlearn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.7/site-packages (from hmmlearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.16->hmmlearn) (0.14.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hmmlearn\n",
    "from msspackages import Pyspark_data_ingestion\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import *\n",
    "from hmmlearn import hmm,base\n",
    "from sklearn.preprocessing import StandardScaler as scale\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# pandas settings \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e234984-446a-4830-a728-af610ba41669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 ms, sys: 22.6 ms, total: 45 ms\n",
      "Wall time: 9.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "node_obj = Pyspark_data_ingestion(year = '2022', month = \"7\", day = \"7\", hour = -1, filter_column_value ='Node',setup='128gb')\n",
    "spark_node = node_obj.get_spark()\n",
    "err, node_data = node_obj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac50f98-8f0b-434b-87a9-eb210962ec93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = node_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320ee49-e7eb-4c8b-b95c-c9ab5c3b2745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00e8a7-779c-4da2-8633-26a87bc6eb7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.InstanceId[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100d424-646b-4631-a075-1a18ab0cc353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df.ClusterName=='nk-ndc-eks-cluster-test-dev-usw2-az2-perf'][['NodeName','Timestamp','node_cpu_utilization','node_memory_utilization']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a98e1a-a8b5-4305-95af-2767720ab760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.dropna(subset=['node_cpu_utilization','node_memory_utilization'])\n",
    "    nodelist = df.NodeName.unique()\n",
    "    subdfs = []\n",
    "    for node in nodelist:\n",
    "        \n",
    "        subdf = df[df.NodeName==node]\n",
    "        sc_m = scale()\n",
    "        sc_c = scale()\n",
    "        c_s = sc_c.fit_transform(subdf['node_cpu_utilization'].values.reshape(-1,1))\n",
    "        m_s = sc_m.fit_transform(subdf['node_memory_utilization'].values.reshape(-1,1))\n",
    "        subdf['node_cpu_utilization'] = c_s\n",
    "        subdf['node_memory_utilization'] = m_s\n",
    "        subdfs.append(subdf)\n",
    "    cleaned = subdfs[0]\n",
    "    \n",
    "    for i in subdfs[1:]:\n",
    "        cleaned = pd.concat([cleaned, i], axis=0)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a30ea8-92fa-4666-a495-ea15b4571611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "new_df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72034295-db32-456c-aad7-c6bcc377c128",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38583e2e-0963-4d3f-a5e2-6cf1e1c0711e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fcd8b-1fdd-4cc3-9e5c-dec6bb9fbab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(len(new_df.NodeName.unique()),figsize = (10,40),sharex=True)\n",
    "i = 0\n",
    "for node in new_df.NodeName.unique():\n",
    "    sub = new_df[new_df.NodeName==node]\n",
    "    ax[i].hist(sub.node_cpu_utilization)\n",
    "    i+=1\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2402e-e39e-4f87-8b1a-ab0ce49af26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8b820-6254-4e9b-ba07-d629b81a2ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d45490-565b-46d7-ba01-fe2dda4c7ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_data(df,data_frac= 0.5,random_state = 1, slice_length = 6):\n",
    "    res_ind = df.sample(frac=data_frac/6, replace=True, random_state=random_state).index.to_list()\n",
    "    for i in res_ind :\n",
    "        for j in range(slice_length):\n",
    "            res_ind.append(i+j)\n",
    "            \n",
    "    return df.iloc[res_ind.sort()]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d380f2-1fb4-4f2b-9ee7-6f758313723e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302b9d1-ca5d-45f9-8bb7-ded75f009c55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_ind = new_df.sample(frac=0.5/6, replace=True, random_state=1).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18a678-ee81-4bc5-b2db-ca6c42df4d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in res_ind :\n",
    "    for j in range(6):\n",
    "        res_ind.append(i+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d124c9-996e-447d-b13d-29f6c18f1527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res_ind[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2385c-cb30-4ec8-b825-e3459916cb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.iloc[[5,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec681c-201b-4782-9f46-5fadecd5ac60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59a7ecf3-1739-47c9-a90e-50e9e20ce229",
   "metadata": {},
   "source": [
    "## model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80dd7cdb-cb17-472e-87df-7ad7a1485607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 ms, sys: 1.03 ms, total: 19.2 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = Pyspark_data_ingestion(year = '2022', month = \"7\", day = \"7\", hour = \"10\", filter_column_value ='Node',setup='128gb')\n",
    "test_node = test.get_spark()\n",
    "err, test_df = test.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55edc667-2870-4eeb-b61c-414de66c83dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-------------------------------+-------------------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------+--------------------+--------------------+-------------+----+-------+--------------------+--------------+----------------+--------------------------+---------------------+--------------------+-------------------+--------------------+-----------------+-------------------+--------------------------------+-----------------------------------+-----------------+-----------------------+---------------------+-------------------+----------------------+-------------------+-----------------------------+---------------+----------------+-----------------+-----------------------+-----------------------+---------------------+-----------------------+----------------------+-----------------------+------------------------+---------------------+-----------------------+----------------------+-----------------------+---------------------------------+---------------------------+---------+\n",
      "|  account_id|      log_group_name|     log_stream_name|           record_id|         stream_name|record_arrival_stream_timestamp|record_arrival_stream_epochtime|log_event_timestamp|log_event_epochtime|        log_event_id|AutoScalingGroupName|   CloudWatchMetrics|         ClusterName|         InstanceId|InstanceType|            NodeName|             Sources|    Timestamp|Type|Version|          kubernetes|node_cpu_limit|node_cpu_request|node_cpu_reserved_capacity|node_cpu_usage_system|node_cpu_usage_total|node_cpu_usage_user|node_cpu_utilization|node_memory_cache|node_memory_failcnt|node_memory_hierarchical_pgfault|node_memory_hierarchical_pgmajfault|node_memory_limit|node_memory_mapped_file|node_memory_max_usage|node_memory_pgfault|node_memory_pgmajfault|node_memory_request|node_memory_reserved_capacity|node_memory_rss|node_memory_swap|node_memory_usage|node_memory_utilization|node_memory_working_set|node_network_rx_bytes|node_network_rx_dropped|node_network_rx_errors|node_network_rx_packets|node_network_total_bytes|node_network_tx_bytes|node_network_tx_dropped|node_network_tx_errors|node_network_tx_packets|node_number_of_running_containers|node_number_of_running_pods|   region|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-------------------------------+-------------------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------+--------------------+--------------------+-------------+----+-------+--------------------+--------------+----------------+--------------------------+---------------------+--------------------+-------------------+--------------------+-----------------+-------------------+--------------------------------+-----------------------------------+-----------------+-----------------------+---------------------+-------------------+----------------------+-------------------+-----------------------------+---------------+----------------+-----------------+-----------------------+-----------------------+---------------------+-----------------------+----------------------+-----------------------+------------------------+---------------------+-----------------------+----------------------+-----------------------+---------------------------------+---------------------------+---------+\n",
      "|346687249423|/aws/containerins...|ip-100-64-19-132....|49627967497190512...|dp-us-west-2-cont...|            2022-07-07 10:06:23|                     1657188383|2022-07-07 10:06:17|      1657188377000|36956535741443506...|nk-ndc-asbc-asg-c...|[{\"Dimensions\":[[...|nk-ndc-eks-cluste...|i-0fd528a5b4db63a58| m5.16xlarge|ip-100-64-19-132....|[\"cadvisor\",\"/pro...|1657188368827|Node|      0|{\"host\":\"ip-100-6...|         64000|            2716|                   4.24375|    405.1504517412961|   684.5653591799788|  388.0779587589941|   1.069633373718717|       5686312960|                  0|                               0|                                  0|     267384934400|              804790272|          21409251328|                  0|                     0|         3340763136|           1.2494208559268776|     2185527296|               0|       7871840256|     1.7896269925370931|             4785192960|    33970.68131851052|                      0|                     0|     138.36062323398951|       67947.01675297646|    33976.33543446595|                      0|                     0|     136.76351905177415|                               18|                         12|us-west-2|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-------------------------------+-------------------------------+-------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------+--------------------+--------------------+-------------+----+-------+--------------------+--------------+----------------+--------------------------+---------------------+--------------------+-------------------+--------------------+-----------------+-------------------+--------------------------------+-----------------------------------+-----------------+-----------------------+---------------------+-------------------+----------------------+-------------------+-----------------------------+---------------+----------------+-----------------+-----------------------+-----------------------+---------------------+-----------------------+----------------------+-----------------------+------------------------+---------------------+-----------------------+----------------------+-----------------------+---------------------------------+---------------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42216b5-5037-4de6-baa1-fc552d8ca5e7",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc182400-684d-4301-8ecb-92bef4ffca94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "features = ['node_cpu_utilization','node_memory_utilization']\n",
    "#filter inital node df based on request features\n",
    "node_df =test_df.select(\"Timestamp\", \"NodeName\", 'node_cpu_utilization','node_memory_utilization')\n",
    "node_df = node_df.withColumn(\"Datetime\",(col(\"Timestamp\")/1000).cast(\"timestamp\"))\n",
    "\n",
    "# Drop NA\n",
    "cleaned_node_df = node_df.na.drop(subset=features)\n",
    "\n",
    "\n",
    "#Quality(timestamp filtered) nodes\n",
    "quality_filtered_node_df = cleaned_node_df.groupBy(\"NodeName\").agg(count(\"Timestamp\").alias(\"timestamp_count\"))\n",
    "quality_filtered_nodes = quality_filtered_node_df.filter(col(\"timestamp_count\").between(45,75))\n",
    "\n",
    "print(3)\n",
    "#Processed Node DF                                                      \n",
    "processed_node_df = cleaned_node_df.filter(col(\"NodeName\").isin(quality_filtered_nodes[\"NodeName\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf94a6d-0178-4fd4-9761-1e61ecbc6e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c35eed1-a35e-4642-a749-4b57d0f836d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_df, test_df = processed_node_df .randomSplit(weights=[0.8,0.2], seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56538a78-b8f5-4551-9497-fe741ea9a1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+-----------------------+--------------------+\n",
      "|    Timestamp|            NodeName|node_cpu_utilization|node_memory_utilization|            Datetime|\n",
      "+-------------+--------------------+--------------------+-----------------------+--------------------+\n",
      "|1657188364512|ip-100-64-71-112....| 0.24938494304307315|     1.2227495249870506|2022-07-07 10:06:...|\n",
      "+-------------+--------------------+--------------------+-----------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.show(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e33c1b-95d1-4801-a235-807179e0e555",
   "metadata": {},
   "source": [
    "### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a67cd3bc-ff8d-40ba-800e-0f7b5e403991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "samplesize =1\n",
    "time_steps=12\n",
    "final_df = np.zeros((samplesize,time_steps,len(features)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f3ed0b3-e1e5-41e4-bbf0-19b948bb7f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea31b948-8fc3-4bd7-9883-cb89029ed4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac297f9f-a317-4f1f-ae76-a9f5d6f98f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "##pick random node\n",
    "random_nodename = random.choice(input_df.select(\"NodeName\").rdd.flatMap(list).collect())\n",
    "node_df = input_df[(input_df[\"NodeName\"] ==  random_nodename)][[\"Timestamp\", \"NodeName\"] + features].select('*')\n",
    "node_df = node_df.sort(\"Timestamp\")\n",
    "node_df = node_df.na.drop(subset=features)\n",
    "    \n",
    "\n",
    "\n",
    "#standardize data from the node\n",
    "\n",
    "\n",
    "w = Window.partitionBy('NodeName')\n",
    "for c in features:\n",
    "    node_df = (node_df.withColumn('mean', F.mean(c).over(w))\n",
    "        .withColumn('stddev', F.stddev(c).over(w))\n",
    "        .withColumn(c, ((F.col(c) - F.col('mean')) / (F.col('stddev'))))\n",
    "        .drop('mean')\n",
    "        .drop('stddev'))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8afa1-502c-4f97-8b8e-662149a32a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6458675d-fc7b-4250-a380-4e9ce324cec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp', 'NodeName', 'node_cpu_utilization', 'node_memory_utilization']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a86197f6-09e5-48ea-8279-a65c39d08983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pick random time slice of 12 timestamps from this node\n",
    "final_df = None\n",
    "start = random.choice(range(node_df.count()-time_steps))\n",
    "node_slice_df = node_df.withColumn('rn', row_number().over(Window.orderBy(\"Timestamp\"))).filter((col(\"rn\") >= start) & (col(\"rn\") < start+time_steps)).select([\"Timestamp\"] + features)\n",
    "node_uti_data = node_slice_df.select(*features).rdd.flatMap(list).collect()\n",
    "\n",
    "if not final_df:\n",
    "    final_df = node_slice_df \n",
    "else:\n",
    "    final_df = final_df.union(node_slice_df )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2078a58e-ab69-40ba-ba76-99537274a518",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp', 'node_cpu_utilization', 'node_memory_utilization']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df.groupBy(\"Timestamp\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86d78e5a-b828-4e64-b318-c27419d69079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vecAssembler2 = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "test_df = vecAssembler2.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8261c468-194b-4478-9f1d-525952e103c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp', 'node_cpu_utilization', 'node_memory_utilization', 'features']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efff14d4-b5ec-4dd3-adc1-1619a2412512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_list = test_df.select(\"features\").rdd.flatMap(list).collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "02068932-7358-4a1d-90cc-6a2f2cc9c41d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.6247, -0.1377]),\n",
       " DenseVector([0.6129, 0.157]),\n",
       " DenseVector([-0.1713, 0.2353]),\n",
       " DenseVector([1.1362, -0.079]),\n",
       " DenseVector([2.2032, 0.1915]),\n",
       " DenseVector([0.6544, 0.5208]),\n",
       " DenseVector([0.744, 0.7314]),\n",
       " DenseVector([0.0606, -0.0444]),\n",
       " DenseVector([0.1141, 0.2514]),\n",
       " DenseVector([0.291, 0.3067]),\n",
       " DenseVector([-0.0118, 0.4379]),\n",
       " DenseVector([0.1642, 2.236])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "907e0c0b-daed-4c18-8d25-e7a53ede88af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp', 'node_cpu_utilization', 'node_memory_utilization']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc71486d-cd8e-427a-9837-a5e0141b37c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'collect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-67ff202939dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnode_slice_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_slice_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_slice_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'collect'"
     ]
    }
   ],
   "source": [
    "\n",
    "node_slice_df = node_slice_df.select\n",
    "final_df[n] = node_slice_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a42794ab-2c91-448b-9e98-5859ad7ccdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp', 'scaled_features']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe795973-4063-41b0-b928-6857710ec12f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'avg(scaled_features)' due to data type mismatch: function average requires numeric or interval types, not struct<type:tinyint,size:int,indices:array<int>,values:array<double>>;\n'Aggregate [Timestamp#1594], [Timestamp#1594, avg(scaled_features#2054) AS scaled_features#2100]\n+- Project [Timestamp#1594, scaled_features#2054]\n   +- Project [Timestamp#1594, scaled_features#2054]\n      +- Project [Timestamp#1594, scaled_features#2054]\n         +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619, vectorized_features#2045, UDF(vectorized_features#2045) AS scaled_features#2054]\n            +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619, UDF(struct(node_cpu_utilization, node_cpu_utilization#1604, node_memory_utilization, node_memory_utilization#1619)) AS vectorized_features#2045]\n               +- Filter atleastnnonnulls(2, node_cpu_utilization#1604, node_memory_utilization#1619)\n                  +- Sort [Timestamp#1594 ASC NULLS FIRST], true\n                     +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619]\n                        +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619]\n                           +- Filter (NodeName#1592 = ip-172-27-9-79.ec2.internal)\n                              +- Sample 0.0, 0.8, false, 200\n                                 +- Sort [Timestamp#1594 ASC NULLS FIRST, NodeName#1592 ASC NULLS FIRST, node_cpu_utilization#1604 ASC NULLS FIRST, node_memory_utilization#1619 ASC NULLS FIRST, Datetime#1863 ASC NULLS FIRST], false\n                                    +- Filter NodeName#1592 IN (NodeName#1592)\n                                       +- Filter atleastnnonnulls(2, node_cpu_utilization#1604, node_memory_utilization#1619)\n                                          +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619, cast((cast(Timestamp#1594 as double) / cast(1000 as double)) as timestamp) AS Datetime#1863]\n                                             +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619]\n                                                +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, AutoScalingGroupName#1587, CloudWatchMetrics#1688, ClusterName#1589, InstanceId#1590, InstanceType#1591, NodeName#1592, Sources#1745, Timestamp#1594, Type#1595, Version#1596, to_json(kubernetes#1597, Some(Etc/UTC)) AS kubernetes#1802, node_cpu_limit#1598L, node_cpu_request#1599L, node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                   +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, AutoScalingGroupName#1587, CloudWatchMetrics#1688, ClusterName#1589, InstanceId#1590, InstanceType#1591, NodeName#1592, to_json(Sources#1593, Some(Etc/UTC)) AS Sources#1745, Timestamp#1594, Type#1595, Version#1596, kubernetes#1597, node_cpu_limit#1598L, node_cpu_request#1599L, node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                      +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, AutoScalingGroupName#1587, to_json(CloudWatchMetrics#1588, Some(Etc/UTC)) AS CloudWatchMetrics#1688, ClusterName#1589, InstanceId#1590, InstanceType#1591, NodeName#1592, Sources#1593, Timestamp#1594, Type#1595, Version#1596, kubernetes#1597, node_cpu_limit#1598L, node_cpu_request#1599L, node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                         +- Filter (Type#1595 = Node)\n                                                            +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, log_event_message#1569.AutoScalingGroupName AS AutoScalingGroupName#1587, log_event_message#1569.CloudWatchMetrics AS CloudWatchMetrics#1588, log_event_message#1569.ClusterName AS ClusterName#1589, log_event_message#1569.InstanceId AS InstanceId#1590, log_event_message#1569.InstanceType AS InstanceType#1591, log_event_message#1569.NodeName AS NodeName#1592, log_event_message#1569.Sources AS Sources#1593, log_event_message#1569.Timestamp AS Timestamp#1594, log_event_message#1569.Type AS Type#1595, log_event_message#1569.Version AS Version#1596, log_event_message#1569.kubernetes AS kubernetes#1597, log_event_message#1569.node_cpu_limit AS node_cpu_limit#1598L, log_event_message#1569.node_cpu_request AS node_cpu_request#1599L, log_event_message#1569.node_cpu_reserved_capacity AS node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                               +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, from_json(StructField(AutoScalingGroupName,StringType,true), StructField(CloudWatchMetrics,ArrayType(StructType(StructField(Dimensions,ArrayType(ArrayType(StringType,true),true),true),StructField(Metrics,ArrayType(StructType(StructField(Name,StringType,true),StructField(Unit,StringType,true)),true),true),StructField(Namespace,StringType,true)),true),true), StructField(ClusterName,StringType,true), StructField(InstanceId,StringType,true), StructField(InstanceType,StringType,true), StructField(NodeName,StringType,true), StructField(Sources,ArrayType(StringType,true),true), StructField(Timestamp,StringType,true), StructField(Type,StringType,true), StructField(Version,StringType,true), StructField(kubernetes,StructType(StructField(host,StringType,true)),true), StructField(node_cpu_limit,LongType,true), StructField(node_cpu_request,LongType,true), StructField(node_cpu_reserved_capacity,DoubleType,true), StructField(node_cpu_usage_system,DoubleType,true), StructField(node_cpu_usage_total,DoubleType,true), StructField(node_cpu_usage_user,DoubleType,true), StructField(node_cpu_utilization,DoubleType,true), StructField(node_memory_cache,LongType,true), StructField(node_memory_failcnt,LongType,true), StructField(node_memory_hierarchical_pgfault,LongType,true), StructField(node_memory_hierarchical_pgmajfault,LongType,true), StructField(node_memory_limit,LongType,true), StructField(node_memory_mapped_file,LongType,true), ... 23 more fields) AS log_event_message#1569, year#1548, month#1549, day#1550, hour#1551, region#1552]\n                                                                  +- Relation [account_id#1537,log_group_name#1538,log_stream_name#1539,record_id#1540,stream_name#1541,record_arrival_stream_timestamp#1542,record_arrival_stream_epochtime#1543L,log_event_timestamp#1544,log_event_epochtime#1545L,log_event_id#1546,log_event_message#1547,year#1548,month#1549,day#1550,hour#1551,region#1552] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1715d3a89783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"scaled_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaled_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaled_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mexprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'avg(scaled_features)' due to data type mismatch: function average requires numeric or interval types, not struct<type:tinyint,size:int,indices:array<int>,values:array<double>>;\n'Aggregate [Timestamp#1594], [Timestamp#1594, avg(scaled_features#2054) AS scaled_features#2100]\n+- Project [Timestamp#1594, scaled_features#2054]\n   +- Project [Timestamp#1594, scaled_features#2054]\n      +- Project [Timestamp#1594, scaled_features#2054]\n         +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619, vectorized_features#2045, UDF(vectorized_features#2045) AS scaled_features#2054]\n            +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619, UDF(struct(node_cpu_utilization, node_cpu_utilization#1604, node_memory_utilization, node_memory_utilization#1619)) AS vectorized_features#2045]\n               +- Filter atleastnnonnulls(2, node_cpu_utilization#1604, node_memory_utilization#1619)\n                  +- Sort [Timestamp#1594 ASC NULLS FIRST], true\n                     +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619]\n                        +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619]\n                           +- Filter (NodeName#1592 = ip-172-27-9-79.ec2.internal)\n                              +- Sample 0.0, 0.8, false, 200\n                                 +- Sort [Timestamp#1594 ASC NULLS FIRST, NodeName#1592 ASC NULLS FIRST, node_cpu_utilization#1604 ASC NULLS FIRST, node_memory_utilization#1619 ASC NULLS FIRST, Datetime#1863 ASC NULLS FIRST], false\n                                    +- Filter NodeName#1592 IN (NodeName#1592)\n                                       +- Filter atleastnnonnulls(2, node_cpu_utilization#1604, node_memory_utilization#1619)\n                                          +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619, cast((cast(Timestamp#1594 as double) / cast(1000 as double)) as timestamp) AS Datetime#1863]\n                                             +- Project [Timestamp#1594, NodeName#1592, node_cpu_utilization#1604, node_memory_utilization#1619]\n                                                +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, AutoScalingGroupName#1587, CloudWatchMetrics#1688, ClusterName#1589, InstanceId#1590, InstanceType#1591, NodeName#1592, Sources#1745, Timestamp#1594, Type#1595, Version#1596, to_json(kubernetes#1597, Some(Etc/UTC)) AS kubernetes#1802, node_cpu_limit#1598L, node_cpu_request#1599L, node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                   +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, AutoScalingGroupName#1587, CloudWatchMetrics#1688, ClusterName#1589, InstanceId#1590, InstanceType#1591, NodeName#1592, to_json(Sources#1593, Some(Etc/UTC)) AS Sources#1745, Timestamp#1594, Type#1595, Version#1596, kubernetes#1597, node_cpu_limit#1598L, node_cpu_request#1599L, node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                      +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, AutoScalingGroupName#1587, to_json(CloudWatchMetrics#1588, Some(Etc/UTC)) AS CloudWatchMetrics#1688, ClusterName#1589, InstanceId#1590, InstanceType#1591, NodeName#1592, Sources#1593, Timestamp#1594, Type#1595, Version#1596, kubernetes#1597, node_cpu_limit#1598L, node_cpu_request#1599L, node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                         +- Filter (Type#1595 = Node)\n                                                            +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, log_event_message#1569.AutoScalingGroupName AS AutoScalingGroupName#1587, log_event_message#1569.CloudWatchMetrics AS CloudWatchMetrics#1588, log_event_message#1569.ClusterName AS ClusterName#1589, log_event_message#1569.InstanceId AS InstanceId#1590, log_event_message#1569.InstanceType AS InstanceType#1591, log_event_message#1569.NodeName AS NodeName#1592, log_event_message#1569.Sources AS Sources#1593, log_event_message#1569.Timestamp AS Timestamp#1594, log_event_message#1569.Type AS Type#1595, log_event_message#1569.Version AS Version#1596, log_event_message#1569.kubernetes AS kubernetes#1597, log_event_message#1569.node_cpu_limit AS node_cpu_limit#1598L, log_event_message#1569.node_cpu_request AS node_cpu_request#1599L, log_event_message#1569.node_cpu_reserved_capacity AS node_cpu_reserved_capacity#1600, ... 32 more fields]\n                                                               +- Project [account_id#1537, log_group_name#1538, log_stream_name#1539, record_id#1540, stream_name#1541, record_arrival_stream_timestamp#1542, record_arrival_stream_epochtime#1543L, log_event_timestamp#1544, log_event_epochtime#1545L, log_event_id#1546, from_json(StructField(AutoScalingGroupName,StringType,true), StructField(CloudWatchMetrics,ArrayType(StructType(StructField(Dimensions,ArrayType(ArrayType(StringType,true),true),true),StructField(Metrics,ArrayType(StructType(StructField(Name,StringType,true),StructField(Unit,StringType,true)),true),true),StructField(Namespace,StringType,true)),true),true), StructField(ClusterName,StringType,true), StructField(InstanceId,StringType,true), StructField(InstanceType,StringType,true), StructField(NodeName,StringType,true), StructField(Sources,ArrayType(StringType,true),true), StructField(Timestamp,StringType,true), StructField(Type,StringType,true), StructField(Version,StringType,true), StructField(kubernetes,StructType(StructField(host,StringType,true)),true), StructField(node_cpu_limit,LongType,true), StructField(node_cpu_request,LongType,true), StructField(node_cpu_reserved_capacity,DoubleType,true), StructField(node_cpu_usage_system,DoubleType,true), StructField(node_cpu_usage_total,DoubleType,true), StructField(node_cpu_usage_user,DoubleType,true), StructField(node_cpu_utilization,DoubleType,true), StructField(node_memory_cache,LongType,true), StructField(node_memory_failcnt,LongType,true), StructField(node_memory_hierarchical_pgfault,LongType,true), StructField(node_memory_hierarchical_pgmajfault,LongType,true), StructField(node_memory_limit,LongType,true), StructField(node_memory_mapped_file,LongType,true), ... 23 more fields) AS log_event_message#1569, year#1548, month#1549, day#1550, hour#1551, region#1552]\n                                                                  +- Relation [account_id#1537,log_group_name#1538,log_stream_name#1539,record_id#1540,stream_name#1541,record_arrival_stream_timestamp#1542,record_arrival_stream_epochtime#1543L,log_event_timestamp#1544,log_event_epochtime#1545L,log_event_id#1546,log_event_message#1547,year#1548,month#1549,day#1550,hour#1551,region#1552] parquet\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.select(\"Timestamp\",\"scaled_features\")\n",
    "train_df.groupby(\"Timestamp\").agg(mean(\"scaled_features\").alias(\"scaled_features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f78804bd-3e3c-41f5-880c-82b7e8fce4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp', 'scaled_features']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29ab62-4b11-4463-9054-2b0fcf6eccf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfaa6ae-b335-41ea-a3b5-5fccbe32bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in features]\n",
    "scalers = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in features]\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline.fit(node_df)\n",
    "scaledData = scalerModel.transform(node_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2276541c-cd37-4d5a-8891-7ea4e7951d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_list = [col + \"_scaled\" for col in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13f15f25-ea43-46cc-b51c-2662c0ac0796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['node_cpu_utilization_scaled', 'node_memory_utilization_scaled']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c320aa3-962a-46d3-8efa-08ebcf7feaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------+------------------------------+\n",
      "|    Timestamp|node_cpu_utilization_scaled|node_memory_utilization_scaled|\n",
      "+-------------+---------------------------+------------------------------+\n",
      "|1657188485695|       [11.840651108770663]|           [139.4979764656397]|\n",
      "+-------------+---------------------------+------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.select('Timestamp','node_cpu_utilization_scaled', 'node_memory_utilization_scaled').show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1a1fc4a-674b-47ab-aa48-79d45c0e5fff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'DataFrame'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7c0b2103b7bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    " #fill the large dataset\n",
    "        node_slice_df = node_slice_df.select\n",
    "        final_df[n] = node_slice_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298fc92-8b59-434f-8502-fe16e2911456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e472872-341b-42b8-afe5-c81354d7f678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d8488-214a-4a8b-a00f-891007b363cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4115d2f-3086-4b11-859d-645afb922e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    n = 0\n",
    "    samplesize = input_df.count*weight/time_step\n",
    "    final_df = np.zeros((samplesize,time_steps,len(features)+1))\n",
    "    while n < samplesize:\n",
    "\n",
    "\n",
    "        ##pick random node\n",
    "        random_nodename = random.choice(input_df.select(\"NodeName\").rdd.flatMap(list).collect())\n",
    "        node_df = input_df[(input_df[\"NodeName\"] ==  random_nodename)][[\"Timestamp\", \"NodeName\"] + features].select('*')\n",
    "        node_df = node_df.sort(\"Timestamp\")\n",
    "        node_df = node_df.na.drop(subset=features)\n",
    "\n",
    "        #fix negative number bug \n",
    "        if node_df.count()-time_steps<= 0:\n",
    "            print(f'Exception occurred: not enough data')\n",
    "            continue\n",
    "            \n",
    "        #standardize data from the node\n",
    "         \n",
    "        assembler = VectorAssembler(inputCols=features, outputCol=\"vectorized_features\")\n",
    "        scaler = StandardScaler(inputCol = \"vectorized_features\", outputCol = \"scaled_features\", withMean=True, withStd=True)\n",
    "        pipeline = Pipeline(stages=[assembler, scaler])\n",
    "        node_df = pipeline.fit(node_df).transform(node_df)\n",
    "    \n",
    "\n",
    "        #pick random time slice of 12 timestamps from this node\n",
    "        start = random.choice(range(node_df.count()-time_steps))\n",
    "        node_slice_df = node_df.withColumn('rn', row_number().over(Window.orderBy(\"Timestamp\"))).filter((col(\"rn\") >= start) & (col(\"rn\") < start+time_steps)).select([\"Timestamp\"] + features)\n",
    "\n",
    "        #fill the large dataset\n",
    "        node_slice_df = node_slice_df.select\n",
    "        final_df[n] = node_slice_df.collect()\n",
    "\n",
    "        print(f'Finished with sample #{n}')\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    final_df.reshape(time_steps*samplesize,len(features)+1)\n",
    " \n",
    "    final_df.groupBy(\"Timestamp\").mean(\"no\").show(truncate=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d5cc5-92bb-413f-8888-ff3614f7e493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b639287-0c3e-40d7-bce1-77b0ec84302d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf040cc-ce00-43ce-aef8-14b8a877d9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb5daf-0baf-4171-a220-1eebd0b3ca8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f28fc-26ad-49a8-be74-fb58b125642d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a742fe-5065-4af7-8d3d-b1ccdf75c2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2b6ab-446c-4d02-bf1b-7fa2a4c9b88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c25c72e-681d-4a37-b72a-57f01368460c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ba2ce-97dc-41e6-89ed-d88ddac16417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "samplesize = 3\n",
    "features = ['node_cpu_utilization','node_memory_utilization']\n",
    "input_df = test_df\n",
    "time_steps = 12\n",
    "final_df = np.zeros((samplesize,time_steps,len(features)+1))\n",
    "while n < samplesize:\n",
    "    ##pick random node\n",
    "    random_nodename = random.choice(input_df.select(\"NodeName\").rdd.flatMap(list).collect())\n",
    "    node_df = input_df[(input_df[\"NodeName\"] ==  random_nodename)][[\"Timestamp\", \"NodeName\"] + features].select('*')\n",
    "    node_df = node_df.sort(\"Timestamp\")\n",
    "    node_df = node_df.na.drop(subset=features)\n",
    "\n",
    "    #fix negative number bug \n",
    "    if node_df.count()-time_steps<= 0:\n",
    "        print(f'Exception occurred: not enough data')\n",
    "        continue\n",
    "\n",
    "    #pick random time slice of 12 timestamps from the random node\n",
    "    start = random.choice(range(node_df.count()-time_steps))\n",
    "    node_slice_df = node_df.withColumn('rn', row_number().over(Window.orderBy(\"Timestamp\"))).filter((col(\"rn\") >= start) & (col(\"rn\") < start+time_steps)).select([\"Timestamp\"] + features)\n",
    "    \n",
    "    #fill the large dataset\n",
    "    final_df[n] = node_slice_df.collect()\n",
    "    \n",
    "    print(f'Finished with sample #{n}')\n",
    "\n",
    "    n += 1\n",
    "\n",
    "final_df = final_df.reshape(time_steps*samplesize,len(features)+1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19dcb96-23da-46fc-a5f2-c04e6399bfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
