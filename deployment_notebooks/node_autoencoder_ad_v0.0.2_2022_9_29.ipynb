{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16904a24-ec04-4368-bf80-bc43dc611f4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wallaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c5c05-6de8-441e-bd3b-41c56645f0b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11f948-2d07-474b-aadf-2a1f9f1b4c24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "wl.list_workspaces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066e61d-12e2-440b-a0cc-7cdce0b360ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# only run first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dcbce-f057-4d15-bce6-4ac6ed9a7117",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#wrk = wl.create_workspace('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a378ea-20e2-48c6-b52d-fb1832724327",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "for w in ws:\n",
    "    print(w.name())\n",
    "    if w.name() == 'dish-pd-eks-ad':\n",
    "        wl.set_current_workspace(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711805dd-fb31-406d-bf11-2e11437a318b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wl.get_current_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b031de7-4257-4ee8-8998-6734fc04572f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Upload and run our model\n",
    "\n",
    "For this example we will be using an open source model that uses an Aloha CNN LSTM model for classifiying Domain names.\n",
    "\n",
    "## Config \n",
    "before deploying an inference engine we will set the configuration of the engine.\n",
    "To do this we will use the wallaroo DeploymentConfigBuilder() and fill in the options listed below to determine what the properties of our inference engine will be\n",
    "\n",
    "note: this will not start the process of building anything in the kubernetes cluster yet. we are just setting the deployment configuration we will want to use later.\n",
    "- replica_count - 1 => when deployed this will have a single inference engine\n",
    "- cpus - 14 => each inference engine will have 14 cpus\n",
    "- memory - 50Gi => each inference engine will have 50 Gb of memory\n",
    "\n",
    "# Recommedations\n",
    "for this we are going to create following deployment_configration: \n",
    " - deployment_config\n",
    " -- this config will use a single replica with 4 cpus and 8 Gb of memory and will be used for our deployment later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114dfc6-21e0-4f9b-ab25-2895be4b89cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(4).memory(\"4Gi\").build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bca96a-23ba-494a-99c0-e2408f71f0ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bc45e-a86e-4f27-ad33-ea1b73328e48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = wl.upload_model('node-autoencoder-ad', \"./node_autoencoder_ad_v0.0.2_training_2022_9_29.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5a05d-22ae-48da-9646-e50b9ca41d29",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deploy a model\n",
    "Now that we have a model that we want to use we will create a deployment for it. \n",
    "\n",
    "We will tell the deployment we are using a tensorflow model and give the deployment name and the configuration we want for the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48887f0-2811-444a-9609-f0e293be73d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a508fa-1054-40a2-aee5-60bd67d45673",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = wl.build_pipeline('node-autoencoder-ad')\n",
    "p = p.add_model_step(model)\n",
    "pipeline = p.deploy(deployment_config = deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3dd21a-d46c-4046-b983-bf9d76c6779e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Successful deployment\n",
    "now that we have a deployment running we start inferencing\n",
    "(if you are still running out of resources check to make sure all other deployments have been taken down)\n",
    "\n",
    "## infer 1 row\n",
    "to test we will infer a single row of data and see the results using the infer_from_file tool\n",
    "we will pass a single encoded url into the inference engine and print the results back out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d723d-a841-449f-b470-6f8909deaf36",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.infer_from_file(\"data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f76eb-62e3-43e6-a1fa-5ffce203984b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run larger batch\n",
    "Now we will run a larger batch. We have prepared a 1,000 inference file and a 25,000 inference file\n",
    "once the results are finished they will be placed in a file titled curl_response.txt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc129b7-0827-48c3-80d6-03f4bc3bd5ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline._deployment._url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57f60e-894e-4e18-ae50-f37876521cc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!curl -X POST {pipeline._deployment._url()} -H \"Content-Type:application/json\" --data @test_data10.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ff6e2-8b5a-4ade-b3c7-00d310e17d8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = wl.upload_model('ccfraud-xgb', './models/xgboost_ccfraud.onnx')\n",
    "pipeline.replace_with_model_step(index=0, model=model2).deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1634d-1198-4cee-ad2c-262135f60088",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Undeploy model\n",
    "This will take down our inference engine. and free up the resources in kubernetes\n",
    "- Note that if the pipeline variable is unchanged deployment.deploy() will restart the inference engine in the same configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c552344-7808-4ea2-a439-224286af93a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}