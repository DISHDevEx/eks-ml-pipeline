{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Multivariate Training (Expiriment 3)\n",
    "\n",
    " Vinayak Sharma\n",
    "  \n",
    " This notebook aims to train an autoencoder on univariate timeseries data from EKS Performance metrics for MANY NODES. The training data is 'non-anomalous'. \n",
    "\n",
    " The model and implementation techniques can be found in the following github: https://github.com/emerelte/kad\n",
    " \n",
    " The research paper that this work is based on can be found here: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9925210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##global variable\n",
    "timesteps = 12\n",
    "time_steps = 12\n",
    "batch_size = 6\n",
    "n_samples = batch_size*100\n",
    "features = ['node_cpu_utilization','node_memory_utilization','node_network_total_bytes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r /root/eks-ml-pipeline/modeling/kad/kad/requirements.txt\n",
    "# # # !pip install keras\n",
    "# # # !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# mpl.rcParams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import json\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from pca_ad_dish_5g import pca_ad_dish_5g\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import _pickle as cPickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets read in our training sample set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_full = pd.read_parquet('/root/healthy_clusters_node_month.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>InstanceId</th>\n",
       "      <th>ClusterName</th>\n",
       "      <th>node_cpu_utilization</th>\n",
       "      <th>node_cpu_limit</th>\n",
       "      <th>node_cpu_request</th>\n",
       "      <th>node_cpu_usage_total</th>\n",
       "      <th>node_memory_utilization</th>\n",
       "      <th>node_memory_request</th>\n",
       "      <th>node_memory_limit</th>\n",
       "      <th>node_network_rx_bytes</th>\n",
       "      <th>node_network_rx_dropped</th>\n",
       "      <th>node_network_rx_errors</th>\n",
       "      <th>node_network_rx_packets</th>\n",
       "      <th>node_network_total_bytes</th>\n",
       "      <th>node_network_tx_bytes</th>\n",
       "      <th>node_network_tx_dropped</th>\n",
       "      <th>node_network_tx_errors</th>\n",
       "      <th>node_network_tx_packets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1654235914008</td>\n",
       "      <td>i-01470d8d8e7b4fd2f</td>\n",
       "      <td>nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01</td>\n",
       "      <td>2.905276</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>116.211031</td>\n",
       "      <td>9.394841</td>\n",
       "      <td>998244352</td>\n",
       "      <td>16476487680</td>\n",
       "      <td>26133.384568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.334264</td>\n",
       "      <td>56498.252063</td>\n",
       "      <td>30364.867495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.300537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654235917104</td>\n",
       "      <td>i-0b36e8825c482f762</td>\n",
       "      <td>nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01</td>\n",
       "      <td>1.836838</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>73.473529</td>\n",
       "      <td>7.296163</td>\n",
       "      <td>851443712</td>\n",
       "      <td>16476487680</td>\n",
       "      <td>5812.096097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.781011</td>\n",
       "      <td>13940.610609</td>\n",
       "      <td>8128.514512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.814785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654232669947</td>\n",
       "      <td>i-01470d8d8e7b4fd2f</td>\n",
       "      <td>nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01</td>\n",
       "      <td>2.950336</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>118.013421</td>\n",
       "      <td>9.399117</td>\n",
       "      <td>998244352</td>\n",
       "      <td>16476487680</td>\n",
       "      <td>24538.169434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.939803</td>\n",
       "      <td>54629.097665</td>\n",
       "      <td>30090.928231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.904324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654232681444</td>\n",
       "      <td>i-0b36e8825c482f762</td>\n",
       "      <td>nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01</td>\n",
       "      <td>1.858162</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>74.326468</td>\n",
       "      <td>7.295069</td>\n",
       "      <td>851443712</td>\n",
       "      <td>16476487680</td>\n",
       "      <td>5451.981961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.206624</td>\n",
       "      <td>13392.905270</td>\n",
       "      <td>7940.923309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.632175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654235970093</td>\n",
       "      <td>i-093473861e74eaf2d</td>\n",
       "      <td>mt-ndc-eks-cluster-dev-mt-use1</td>\n",
       "      <td>0.942944</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>150.871041</td>\n",
       "      <td>4.956222</td>\n",
       "      <td>998244352</td>\n",
       "      <td>65902239744</td>\n",
       "      <td>42229.503040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251.934982</td>\n",
       "      <td>90291.404482</td>\n",
       "      <td>48061.901442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251.558398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Timestamp           InstanceId  \\\n",
       "0  1654235914008  i-01470d8d8e7b4fd2f   \n",
       "1  1654235917104  i-0b36e8825c482f762   \n",
       "2  1654232669947  i-01470d8d8e7b4fd2f   \n",
       "3  1654232681444  i-0b36e8825c482f762   \n",
       "4  1654235970093  i-093473861e74eaf2d   \n",
       "\n",
       "                                 ClusterName  node_cpu_utilization  \\\n",
       "0  nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01              2.905276   \n",
       "1  nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01              1.836838   \n",
       "2  nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01              2.950336   \n",
       "3  nk-ndc-eks-cluster-dev-usw2-az1-dev-ncm01              1.858162   \n",
       "4             mt-ndc-eks-cluster-dev-mt-use1              0.942944   \n",
       "\n",
       "   node_cpu_limit  node_cpu_request  node_cpu_usage_total  \\\n",
       "0          4000.0            1266.0            116.211031   \n",
       "1          4000.0            1066.0             73.473529   \n",
       "2          4000.0            1266.0            118.013421   \n",
       "3          4000.0            1066.0             74.326468   \n",
       "4         16000.0            1266.0            150.871041   \n",
       "\n",
       "   node_memory_utilization  node_memory_request  node_memory_limit  \\\n",
       "0                 9.394841            998244352        16476487680   \n",
       "1                 7.296163            851443712        16476487680   \n",
       "2                 9.399117            998244352        16476487680   \n",
       "3                 7.295069            851443712        16476487680   \n",
       "4                 4.956222            998244352        65902239744   \n",
       "\n",
       "   node_network_rx_bytes  node_network_rx_dropped  node_network_rx_errors  \\\n",
       "0           26133.384568                      0.0                     0.0   \n",
       "1            5812.096097                      0.0                     0.0   \n",
       "2           24538.169434                      0.0                     0.0   \n",
       "3            5451.981961                      0.0                     0.0   \n",
       "4           42229.503040                      0.0                     0.0   \n",
       "\n",
       "   node_network_rx_packets  node_network_total_bytes  node_network_tx_bytes  \\\n",
       "0                87.334264              56498.252063           30364.867495   \n",
       "1                22.781011              13940.610609            8128.514512   \n",
       "2                86.939803              54629.097665           30090.928231   \n",
       "3                21.206624              13392.905270            7940.923309   \n",
       "4               251.934982              90291.404482           48061.901442   \n",
       "\n",
       "   node_network_tx_dropped  node_network_tx_errors  node_network_tx_packets  \n",
       "0                      0.0                     0.0                86.300537  \n",
       "1                      0.0                     0.0                22.814785  \n",
       "2                      0.0                     0.0                86.904324  \n",
       "3                      0.0                     0.0                21.632175  \n",
       "4                      0.0                     0.0               251.558398  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Timestamp','InstanceId','node_cpu_utilization','node_memory_utilization','node_network_total_bytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "training_df = training_df_full.drop(training_df_full.columns.difference(columns_to_keep),1, inplace=False)\n",
    "training_df['Timestamp'] = pd.to_datetime(training_df['Timestamp'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2636965"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = training_df.copy()\n",
    "training_df = training_df[training_df.InstanceId != 'i-0b36e8825c482f762']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN DATASET: Build out Dataset for training on many nodes\n",
    "\n",
    "now we are starting to build out a training dataset for two nodes. This will prove our understanding of how to model can generalize across many EC2 instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for normalization\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 12, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "instance_dfs =[]\n",
    "for instance in training_df['InstanceId'].unique():\n",
    "    instance_dfs.append(training_df[training_df.InstanceId == instance].sort_values(by='Timestamp')\\\n",
    "                        .reset_index(drop=True))\n",
    "\n",
    "import random \n",
    "\n",
    "x_train = np.zeros((n_samples,time_steps,len(features)))\n",
    "for b in range(n_samples):\n",
    "    \n",
    "    ##pick random df, and normalize\n",
    "    df = random.choice(instance_dfs)\n",
    "    df = df.drop(columns = ['InstanceId'])\n",
    "    df = df.set_index('Timestamp')\n",
    "    df = df.sort_index()\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "    \n",
    "    \n",
    "    \n",
    "    sample = np.zeros((n_samples,len(features)))\n",
    "    ##make sure length of df is atleast 40\n",
    "    first_time = random.choice(range(len(df)-time_steps))\n",
    "    df.head()\n",
    "    sample = df[features].iloc[first_time:first_time+time_steps]\n",
    "    x_train[b] = sample\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our xtrain shape has the following properties: (sample size, time steps, numFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATASET: Build out Dataset for testing on one node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df.InstanceId == 'i-0b36e8825c482f762']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>InstanceId</th>\n",
       "      <th>node_cpu_utilization</th>\n",
       "      <th>node_memory_utilization</th>\n",
       "      <th>node_network_total_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-03 05:58:37.104</td>\n",
       "      <td>i-0b36e8825c482f762</td>\n",
       "      <td>1.836838</td>\n",
       "      <td>7.296163</td>\n",
       "      <td>13940.610609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-03 05:04:41.444</td>\n",
       "      <td>i-0b36e8825c482f762</td>\n",
       "      <td>1.858162</td>\n",
       "      <td>7.295069</td>\n",
       "      <td>13392.905270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-06-03 05:59:35.385</td>\n",
       "      <td>i-0b36e8825c482f762</td>\n",
       "      <td>1.824157</td>\n",
       "      <td>7.296387</td>\n",
       "      <td>14073.509120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-06-03 05:05:34.448</td>\n",
       "      <td>i-0b36e8825c482f762</td>\n",
       "      <td>1.821361</td>\n",
       "      <td>7.297928</td>\n",
       "      <td>14987.560707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-06-03 06:00:39.866</td>\n",
       "      <td>i-0b36e8825c482f762</td>\n",
       "      <td>1.811754</td>\n",
       "      <td>7.299718</td>\n",
       "      <td>13253.130119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Timestamp           InstanceId  node_cpu_utilization  \\\n",
       "1  2022-06-03 05:58:37.104  i-0b36e8825c482f762              1.836838   \n",
       "3  2022-06-03 05:04:41.444  i-0b36e8825c482f762              1.858162   \n",
       "9  2022-06-03 05:59:35.385  i-0b36e8825c482f762              1.824157   \n",
       "11 2022-06-03 05:05:34.448  i-0b36e8825c482f762              1.821361   \n",
       "17 2022-06-03 06:00:39.866  i-0b36e8825c482f762              1.811754   \n",
       "\n",
       "    node_memory_utilization  node_network_total_bytes  \n",
       "1                  7.296163              13940.610609  \n",
       "3                  7.295069              13392.905270  \n",
       "9                  7.296387              14073.509120  \n",
       "11                 7.297928              14987.560707  \n",
       "17                 7.299718              13253.130119  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## drop the instanceId\n",
    "test_df = test_df.drop(\"InstanceId\",1, inplace=False)\n",
    "\n",
    "\n",
    "##set timestamp as the index\n",
    "test_df = test_df.set_index('Timestamp')\n",
    "\n",
    "##normalize test_df \n",
    "test_df[features] = scaler.fit_transform(test_df[features])\n",
    "\n",
    "\n",
    "##ensure the data is sorted!!\n",
    "test_df = test_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pca_ad_dish_5g():\n",
    "    \"\"\"\n",
    "    @:constructor takes in timesteps, batch size, learning rate, and a train_valid ratio\n",
    "    \n",
    "    @:returns object of class \n",
    "    \"\"\"\n",
    "    ## constructor takes in timesteps, batch size, learning rate, and a train_valid ratio\n",
    "    ## timesteps is the number of time intervals inside of a sample\n",
    "    ## batch size is number of  samples per iteration\n",
    "    ## learning rate is the hyperparameter eta\n",
    "    ## train_valid_ratio indicates the training and validation split crafted from the training set. IE the input dataset will be split for training and validation\n",
    "    \n",
    "    def __init__(self,num_of_features =3, number_of_temporal_slices = 6, timesteps_per_slice = 6, n_modes_to_delete=1):\n",
    "        \n",
    "        ## super().__init__() allows for inheritence amongst child classes\n",
    "        super().__init__()\n",
    "        \n",
    "        ##define num_of_features\n",
    "        self.num_of_features = num_of_features\n",
    "            \n",
    "        ##define number of temporal slices\n",
    "        self.number_of_temporal_slices = number_of_temporal_slices\n",
    "            \n",
    "        ##set timesteps\n",
    "        self.timesteps_per_slice = timesteps_per_slice\n",
    "        \n",
    "        self.N = number_of_temporal_slices * timesteps_per_slice\n",
    "        \n",
    "        ##create a results df\n",
    "        self.results = None\n",
    "        \n",
    "        ##create an x_train\n",
    "        self.x_train: np.ndarray = None\n",
    "      \n",
    "        ##create an x_train\n",
    "        self.n_modes_to_delete= n_modes_to_delete\n",
    "            \n",
    "        ## CREATE the vectors for saving\n",
    "        self.vs = None\n",
    "        \n",
    "        ##create encode and decode for \n",
    "        self.encode_decode_maps = None\n",
    "        \n",
    "        self.ss = None\n",
    "        \n",
    "    def two_time_slice(self, samples):\n",
    "        rank4_sliced = samples.reshape(samples.shape[0],\n",
    "                                       self.number_of_temporal_slices,\n",
    "                                       self.timesteps_per_slice,\n",
    "                                       self.num_of_features,\n",
    "                                      )[:]\n",
    "        \n",
    "        rank3_sliced = rank4_sliced.reshape(samples.shape[0]*self.number_of_temporal_slices,\n",
    "                                            self.timesteps_per_slice,\n",
    "                                            self.num_of_features,\n",
    "                                           )[:]\n",
    "        return rank3_sliced, rank4_sliced\n",
    "\n",
    "    def train(self, x_train):\n",
    "        \"\"\"\n",
    "        @:param x_train: training data \n",
    "        Takes training set and:\n",
    "            - fits the model\n",
    "        @:returns nothing \n",
    "        \"\"\"\n",
    "        \n",
    "        ##log that the autoencoder model training has begun\n",
    "        logging.info(\"Autoencoder model training started\")\n",
    "        \n",
    "        trainX_slices_as_samples, trainX_sliced  =  self.two_time_slice(x_train)\n",
    "        \n",
    "        \n",
    "        self.ss = StandardScaler()\n",
    "  \n",
    "        # initializing, just for shape. The [:] is needed to have a copy instead of a view\n",
    "        trainX_slices_as_samples_ss = trainX_slices_as_samples[:] \n",
    "        # initialize, since I have to do matmult by component\n",
    "        trainX_slices_as_samples_ss_encoded = np.zeros(shape = (self.timesteps_per_slice - self.n_modes_to_delete ,trainX_slices_as_samples.shape[0],self.num_of_features ))\n",
    "        trainX_slices_as_samples_ss_decoded = np.zeros(shape = (trainX_slices_as_samples.shape[0],self.timesteps_per_slice,self.num_of_features))\n",
    "\n",
    "        # one feature at a time:\n",
    "        for i in range(self.num_of_features):\n",
    "            trainX_slices_as_samples_ss[:,:,i] = self.ss.fit_transform(trainX_slices_as_samples[:,:,i]) \n",
    "\n",
    "         \n",
    "        # a list, one component for each feature, with Principal Vectors in a matrix V \n",
    "        vs = []\n",
    "        pca = PCA(n_components = self.timesteps_per_slice)\n",
    "        for i in range(self.num_of_features):\n",
    "            pca.fit(trainX_slices_as_samples_ss[:,:,i]) # fits vectors across (n_samples, n_features)\n",
    "            vs.append(pca.components_) # orthonormal basis of sample space in order of variance explained\n",
    "         \n",
    "        self.vs = vs\n",
    "        \n",
    "        ##create the encode decode maps\n",
    "        self.encode_decode_maps = [np.matmul( vs[i][:-self.n_modes_to_delete,:].T, vs[i][:-self.n_modes_to_delete,:] ) for i in range(self.num_of_features) ]\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##encode and decode using pca\n",
    "        for i in range(self.num_of_features):\n",
    "            trainX_slices_as_samples_ss_encoded[:,:,i] = np.matmul(vs[i][:-self.n_modes_to_delete,:], \n",
    "                                                          trainX_slices_as_samples_ss[:,:,i].T)\n",
    "            trainX_slices_as_samples_ss_decoded[:,:,i] = np.matmul(vs[i][:-self.n_modes_to_delete,:].T, \n",
    "                                                          trainX_slices_as_samples_ss_encoded[:,:,i]).T\n",
    "            \n",
    "         \n",
    "    \n",
    "        # calculate residuals and errors\n",
    "        residuals = trainX_slices_as_samples_ss - trainX_slices_as_samples_ss_decoded\n",
    "        ed_errors = np.linalg.norm(residuals,\n",
    "                    ord =1, # MAE, as in the rules\n",
    "                    axis=1)\n",
    "        \n",
    "        return residuals,ed_errors,self.encode_decode_maps\n",
    "\n",
    "    def test(self, x_test):\n",
    "        \"\"\"\n",
    "        @:param x_test: test data. Must be of shape [ samples, timesteps, features]\n",
    "        \n",
    "        -apply inferencing\n",
    "        \n",
    "        @:returns decoding,residuals\n",
    "        \"\"\"\n",
    "        testX_slices_as_samples, testX_sliced = self.two_time_slice(x_test)\n",
    "        \n",
    "        testX_slices_as_samples_ss = testX_slices_as_samples[:]\n",
    "        \n",
    "                # initialize, since I have to do matmul by feature\n",
    "        testX_slices_as_samples_ss_encoded = np.zeros(shape = (self.timesteps_per_slice - self.n_modes_to_delete ,testX_slices_as_samples.shape[0],self.num_of_features ))\n",
    "        testX_slices_as_samples_ss_decoded = np.zeros(shape = (testX_slices_as_samples.shape[0],self.timesteps_per_slice,self.num_of_features))\n",
    "        \n",
    "        ##apply standard scaler\n",
    "        for i in range(self.num_of_features):\n",
    "            testX_slices_as_samples_ss[:,:,i] = self.ss.fit_transform(testX_slices_as_samples[:,:,i])\n",
    "\n",
    "            \n",
    "       \n",
    "    \n",
    "        ##encode and decode using pca\n",
    "        for i in range(self.num_of_features):\n",
    "            testX_slices_as_samples_ss_encoded[:,:,i] = np.matmul(self.vs[i][:-self.n_modes_to_delete,:], \n",
    "                                                          testX_slices_as_samples_ss[:,:,i].T)\n",
    "            testX_slices_as_samples_ss_decoded[:,:,i] = np.matmul(self.vs[i][:-self.n_modes_to_delete,:].T, \n",
    "                                                          testX_slices_as_samples_ss_encoded[:,:,i]).T\n",
    "            \n",
    "         \n",
    "    \n",
    "        \n",
    "        \n",
    "        residuals = testX_slices_as_samples_ss - testX_slices_as_samples_ss_decoded\n",
    "        \n",
    "        return testX_slices_as_samples_ss_decoded,residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = pca_ad_dish_5g(num_of_features=3, number_of_temporal_slices=3, timesteps_per_slice=4, n_modes_to_delete=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals,ed_errors,encode_decode_maps = pca_model.train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "residuals_reshaped = residuals.reshape(600,12,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = []\n",
    "res_test = []\n",
    "\n",
    "for i in range(0,12,timesteps):\n",
    "    if(i + timesteps < len(test_df)):\n",
    "        sample_topredict_on = test_df.iloc[i:i+timesteps]\n",
    "        x_test = np.array(sample_topredict_on.to_numpy())\n",
    "        x_test = x_test.reshape(1,-1,3)\n",
    "        y_hat,residuals = pca_model.test(x_test)\n",
    "        y_hat_reshape = y_hat.reshape(1,12,3) \n",
    "        res_reshape = residuals.reshape(1,12,3) \n",
    "        y_hat_test.append(y_hat_reshape)\n",
    "        res_test.append(res_reshape)\n",
    "\n",
    "#         print(y_hat.shape)\n",
    "#         preds,errs,anom_scores = model.test(x_test)\n",
    "        \n",
    "#         predictions_f1.append(np.array(preds[:,:,0]))\n",
    "#         predictions_f2.append(preds[:,:,1])\n",
    "#         predictions_f3.append(preds[:,:,2])\n",
    "        \n",
    "#         anomaly_scoresf1.append(np.array(anom_scores[:timesteps,:]))\n",
    "#         anomaly_scoresf2.append(anom_scores[timesteps:timesteps+timesteps,:])\n",
    "#         anomaly_scoresf3.append(anom_scores[timesteps+timesteps:timesteps+timesteps+timesteps,:])\n",
    "        \n",
    "        \n",
    "#         errors_f1.append(np.array(errs[:,:,0]))\n",
    "#         errors_f2.append(errs[:,:,1])\n",
    "#         errors_f3.append(errs[:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1,12,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
